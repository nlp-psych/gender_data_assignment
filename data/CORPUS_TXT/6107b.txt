<abstract><heading>1. Abstract</heading>This report documents a model of communication system developed by Matlab and Simulink, in which various digital modulation schemes used and tested with relative performance easily compared in the implementation part. In order to investigate thoroughly, better methods and solutions are added to the system one by one, through which we can see an improved performance as well as a more complicated communication system. The analysis and discussion part gives an academic support for the results through comparisons and contracts and specifies merits and disadvantages of each scheme from both practical and theoretical aspects. Other features and technologies that may be applicable in the near future are mentioned and considered in the conclusion part together with an overall understanding of this gradually developed system. </abstract><heading>2. Introduction</heading>This report provides a framework for understanding and evaluating the key elements and various methods involved in developing a DSP communication system. There are three parts in a modern digital communication link, which are the transmitter (TX), transmission channel and the receiver (RX). The transmitter processes a message signal in order to produce a signal most likely to pass reliably and efficiently through the channel. So this usually includes modulation of a carrier signal by the information signal, coding of the signal to help correct for transmission errors, filtering of the message or modulated signal to constrain the occupied bandwidth, and power amplification to overcome channel losses. Inversely, the receiver function is principally to reverse the modulation processing in order to recover the message signal, attempting to compensate for any signal degradation introduced by the channel, which normally involve amplification, filtering, demodulation and decoding. The transmission channel is loosely defined as the electrical medium between source and destination and characterized by its attenuation, bandwidth, noise, interference and distortion. In this report, we will concerned with choice of modulation method crucially affects the ease of implementation and the noise tolerance. These forms of digital data modulation such as BPSK, QPSK, 16PSK, 16QAM and 64QAM will be discussed later in the background part, tested and applied in the implementation part. With their BER or PER verse SNR plotted in a same diagram, we can get a relative performance to make research and draw conclusions about these various modulation solutions. The analysis will be done in two ways: one is between the M-ary PSK and single binary signaling (BPSK), the other is between phase shift keying (PSK) and quadrature amplitude modulation (QAM), which need to handle both amplitude and phase information. As the system developing, convolutional coding is added to protect the data transmitted by adding redundancy. Convolutional contains blocks that implement convolutional encoding and decoding. We will test simple and NASA code, gradually placing state pinning, puncturing and packet data with CRC into the system. Different modeling schemes are given and bit/packet error rate performance are compared and contrasted along with theoretical grounding for the relevant results. It is important to trade off the merits and disadvantages of these solutions in different practical situation, such as transmission speed (band rate), reliability (noise immunity), power (linear amplitude) and complexity issues. When it comes to multi-carrier modulation (OFDM), the system gets more powerful and complicated. So we use the IEEE 802.11a(Wireless LAN) model shipped with Matlab. When running this model, we can not only recognize the transmitter, channel and receiver parts clearly but also acquire a better understanding about each component's function. By examining the contents of the modulator banks, we will find that the NASA coding is used. By noticing how the OFDM data, the pilots and the training sequences are collected together and how the padding of 11 zeros at the end of each frame with the addition of the cyclic prefix, we will find that almost all solutions that we have tested and studied before are combined together in this advanced communication system, where each element works efficiently. In the no fading channel, by changing the channel SNR, it is easy to find out SNR range for each modulation scheme. Likely, we will also plot the PER against SNR in three propagation channels and indicate which portion of the SNR uses which modulation scheme and their separate data rate. In a word, this implementation is a general application of all the discussed methods in previous parts. After the whole process, further issues and problems which arised with the practical application are discussed and analyzed in the conclusion part. For instance, the preference to more reliable slow data transmission or faster but less noise immunity transmission; the choice between PSK and QAM; the tradeoff between simpler coding schemes with CRC and retransmission of failed packets and more complicated coding schemes where no retransmission is possible; the decision of modulation schemes with a required SNR and a notionally acceptance PER; the other features needed when modeling the wireless communication and the other modulation and coding schemes that aren't discussed but also popular in digital data communication systems .etc. <heading>3. Background research</heading><heading>3.1 Noise, interference and distortion in channel</heading>Noise is characterized as random, unpredictable electrical signals from a variety of natural sources, for example thermal noise, shot noise and so on. Because of the multiplicity of noise in a communication link, it is hard to define the frequency range, amplitude and instantaneous phase of noise and hence equally difficult to reduce its effect on the performance. The electronic systems generate their own noise due to the random contributions from individual electrons. A good model of the noise generated in electronic system is provided by the 0-mean Gaussian Noise Probability density function，which is a zero-mean, Gaussian random variable. This means that its amplitude at a particular time has a probability density function given by equation. The statement that noise is zero-mean says that, on average, the noise signal takes the value zero. The mean power in the noise signal is equal to the variance of this function. The ratio of signal strength to the noise level is called the signal to noise ratio (SNR). If the SNR is high, few errors will occur. However, as the SNR reduces, the noise may cause symbols to be demodulated incorrectly, and errors will occur. For convenience, most indeed practicing engineers assume noise to fall predominantly into the class of Additive White Gaussian Noise (AWGN) which does form an adequate classification for most cases. In our implementation, we use the AWGN Channel block simulates transmission over a noisy channel <figure/>In communications, the AWGN channel model is one in which the only impairment is the linear addition of white noise with a constant spectral density (expressed as in Figure1.1 a) and a Gaussian distribution of amplitude (expressed as in Figure1.1 b). Because it produces simple, tractable mathematical models which are useful for gaining insight into the underlying behavior of a system before these other phenomena are considered. During the whole implementation process, we will apply the AWGN as a noise generator to add noise in addition to the signal being passed through the digital communication system. However, Even if the AWGN channel is often used as a reference channel model in digital communication systems, it is not sufficient to describe all real channels, such as, suffer from inter-symbol interference, fading, frequency selectivity, interference, nonlinearity or dispersion. Interference arises owing to contamination of the channel by extraneous signals, for example from power lines, ignition systems, machinery, other channel users and so on. It is the result of other man-made radio transmissions. Adjacent channel interference occurs when energy from a carrier spills over into adjacent channels. Co-channel interference occurs when another transmission on the same carrier frequency affects the receiver. This will often arise from transmission in another cell in their network. It is often impulse-like in nature which contains energy over a very wide bandwidth. Unlike noise, if the characteristics are known, then the interference can often be suppressed by filtering or subtraction. Distortion can be introduced within the transmitter, the receiver and the channel. The most common types of link distortion are frequency-dependent phase shift, giving rise to differential group delay, gain variations with time as experienced in a radio channel or with frequency caused by the channel filtering effect and frequency offsets between transmitter and receiver due to Doppler shift or local oscillator errors. In some cases distortion can be corrected using channel equalizers, and gain and frequency control systems. Unlike the two above, distortion disappears when the signal is turned off. Because we will mention the concept "bandwidth efficiency" many times in the following sessions, it is necessary to introduce the Shannon Hartley Capacity Theorem: For error free communication, it is possible to define the capacity which can be supported in an AWGN channel:  FORMULA  Where  FORMULA  (bits per second), w = bandwidth of the modulation baseband signal (Hz)  FORMULA   FORMULA   FORMULA   FORMULA  <heading>3.2 Various digital modulation schemes</heading>First, what is modulation? The process of converting information so that it can be successfully sent through a medium is called modulation. We begin our discussion about digital modulation methods by starting with the introduction of these three basic types. They are: Amplitude shift keying (ASK), Frequency shift keying (FSK) and Phase shift keying (PSK). As we can see from the term, all of these techniques vary a parameter of a sinusoid represent the information which we wish to send, that is, amplitude, frequency and phase. Modulation is a process of facilitating and mapping which takes on signals and converts it into some aspects of a sine wave and then transmits the sine wave, leaving the actual signal behind. The sine wave on the other side is remapped back to a near copy of the signal, which is called demodulation. The simplest form of bandpass data modulation is ASK. The symbols are represented as various discrete amplitudes of a fixed frequency carrier oscillator. Likewise, FSK conveys the data using distinct carrier frequencies to represent symbol states while the amplitude of the modulated wave is constant. FSK has been the most widely used form, being simple both to generate and to detect, and also being insensitive to amplitude fluctuations in the channel. With PSK, the information is contained in the instantaneous phase of the modulated carrier. Usually this phase is imposed and measured with respect to a fixed carrier of known phase. For binary PSK (BPSK), phase states of 0 and 180 are used. BPSK is the simplest form of PSK. It uses two phases which are separated by 180° and so can also be termed 2-PSK. It does not particularly matter exactly where the constellation points are positioned, and in the figure 1.2a they are shown on the real axis, at 0° and 180°. This modulation is the most robust of all the PSKs since it takes serious distortion to make the demodulator reach an incorrect decision. It is, however, only able to modulate at 1bit/symbol and so is unsuitable for high data-rate applications. QPSK sometimes known as 4-PSK, QPSK uses four points on the constellation diagram, equally spaced around a circle. With four phases, QPSK can encode two bits per symbol - twice the rate of BPSK. Analysis shows that this may be used either to double the data rate compared to a BPSK system while maintaining the bandwidth of the signal or to maintain the data-rate of BPSK but halve the bandwidth needed. Since it's both easy to implement and fairly resistant to noise, QPSK is used primarily for sending data from the cable subscriber upstream to the Internet and currently the most widely used modulation types in both wired and wireless modems. Although QPSK can be viewed as a quaternary modulation, it is easier to see it as two independently modulated quadrature carriers. With this interpretation, the even (or odd) bits are used to modulate the in-phase (I) component of the carrier, while the odd (or even) bits are used to modulate the quadrature-phase (Q) component of the carrier. BPSK is used on both carriers and they can be independently demodulated. 16PSK using 16 points on the constellation diagram, equally spaced around a circle, which is a higher order PSK. Increasing the number of symbol states for M-ary PSK beyond four allows further improvements in bandwidth efficiency and data rate, but the additional symbol syayes are no longer orthogonal since they don't lie on the sine or cosine axis of the constellation diagram. It is also more susceptible to noise than similar modulation techniques, such as BPSK and QPSK due to the less space between symbol states. It will be seen in the later part that higher-order modulations exhibit higher error-rates; in exchange however they deliver a higher raw data-rate. A convenient way to represent PSK schemes is on a constellation diagram. This shows the points in the Argand plane where, the real and imaginary axes are termed the in-phase (I) and quadrature (Q) axes respectively due to their 90° separation. Such a representation on perpendicular axes lends itself to straightforward implementation. The amplitude of each point along the in-phase axis is used to modulate a cosine wave and the amplitude along the quadrature axis to modulate a sine wave. In PSK, the constellation points chosen are usually positioned with uniform angular spacing around a circle. This gives maximum phase-separation between adjacent points and thus the best immunity to corruption. They are positioned on a circle so that they can all be transmitted with the same energy. In this way, the modulation of the complex numbers they represent will be the same and thus so will the amplitudes needed for the cosine and sine waves. Two common examples are BPSK, which uses two phases, and QPSK, which uses four phases, although any number of phases may be used. Since the data to be conveyed are usually binary, the PSK scheme is usually designed with the number of constellation points being a power of 2. <heading>Quadrature Amplitude Modulation (QAM). </heading>To data, we have considered only single property modulators using either phase, amplitude or frequency symbols for conveying the data. We may consider that a modulation combining two or more symbol types could give improved performance in the inevitable tradeoff between bandwidth efficiency and noise performance and this is indeed the case. The most commonly one is sometimes classed as Quadrature Amplitude Modulation (QAM), a combination of PSK and ASK. The scheme modulates the signal onto a sequence of complex numbers that lie on a lattice of points in the complex plane, called the constellation of the signal. In QAM, the constellation points are usually arranged in a square grid with equal vertical and horizontal spacing, although other configurations are possible. Since in digital telecommunications the data are usually binary, the number of points in the grid is usually a power of 2 (2,4,8...). Since QAM is usually square, some of these are rare - the most common forms are 16-QAM, 64-QAM, 128-QAM and 256-QAM. By moving to a higher-order constellation, it is possible to transmit more bits per symbol. However, if the mean energy of the constellation is to remain the same, the points must be closer together and are thus more susceptible to noise and other corruption; this results in a higher bit error rate and so higher-order QAM can deliver more data less reliably than lower-order QAM. If data-rates beyond those offered by 16-PSK are required, it is more usual to move to the better alternative---QAM since it achieves a greater distance between adjacent points in the I-Q plane by distributing the points more evenly (Figure 1.2). The complicating factor is that the points are no longer all the same amplitude and so the demodulator must now correctly detect both phase and amplitude, rather than just phase. <figure/><heading>3.3 Convolutional Coding</heading>In telecommunications, an error ratio is the ratio of the number of bits, elements, characters, or blocks incorrectly received to the total number of bits, elements, characters, or blocks sent during a specified time interval. The error ratio is usually expressed in scientific notation; for example, 2.5 erroneous bits out of 100,000 bits transmitted would be 2.5 out of 105 or 2.5 × 10-5. The most commonly encountered ratio is the bit error ratio (BER) - also sometimes referred to as bit error rate. The BER is an indication of the quality of the link, that is, how often a packet or other data unit has to be retransmitted because of an error. Usually, a BER of 10-3 is considered acceptable for a voice connection, and 10-9 for a data link. Too high a BER may indicate that a slower data rate would actually improve overall transmission time for a given amount of transmitted data since the BER might be reduced, lowering the number of packets that had to be resent. Convolutional coding is one major type of channel coding for error correction. The aim of channel encoding is to find codes which transmit quickly, contain many valid code words and can correct or at least detect many errors. Different codes are appropriate for different applications. Deep space communications are limited by the thermal noise of the receiver which is more of a continuous nature than a burst nature. Likewise, narrowband modems are limited by the noise present in the telephone network and is also modeled better as a continuous disturbance. Cell phones are troubled by rapid fading. The high frequencies used can cause rapid fading of the signal even if the receiver is moved a few inches. There are classes of channel codes that are designed to combat fading. A typical music CD uses a powerful Reed-Solomon code to correct for scratches and dust. Cell phones also use powerful coding technique to correct for the fading and noise of high frequency radio transmission. Data modems, telephone transmission and of course NASA all employ powerful channel coding to get the bits through. Convolutional coding is a special case of error-control coding. Unlike a block coder, a convolutional coder is not a memoryless device. Even though a convolutional coder accepts a fixed number of message symbols and produces a fixed number of code symbols, its computations depend not only on the current set of input symbols but on some of the previous input symbols. In our coding process, we will use two coding scheme: simple and NASA convolutional code. The simple structure,  FORMULA  and NASA structure,  FORMULA . Convolutional codes are used in voiceband modems (V.32, V.17, V.34) and in GSM mobile phones, as well as satellite and military communication devices. It is implemented on a bit-by-bit (serial) basis for the incoming source data stream. The encoder has memory and executes an algorithm using a predefined number of the most recent bits to yield the new coded output sequence. The strong redundancy introduced by the basic convolutional coding allows a very powerful error correction. This can be necessary with a very low signal to noise ratio (SNR) at the input to the receiver, but it reduces by a factor of 2 the spectral efficiency of the channel. In this case, the X and Y output streams from the convolutional encoder are applied directly to the I and Q inputs of the modulator for a satellite transmission, and the useful bit-rate of the channel is half the transmitted bit-rate. <figure/>The decoding process is also usually a serial process based on present and previous received data bits or symbols. Both the encoder and decoder can be implemented using recursive engines, with one of the most efficient and well known being the Viterbi convolutional decoder, which we will use in the modeling. The Viterbi algorithm is the optimum algorithm used to decode convolutional codes. There are simplifications to reduce the computational load. They rely on searching only the most likely paths. Although not optimum, they have generally found to give good results in the lower noise environments. Modern microprocessors are capable of implementing these reduced search algorithms at rate greater than 4000 codewords/s. Fundamentally, convolutional codes do not offer more protection against noise than an equivalent block code. In many cases, they generally offer greater simplicity of implementation over a block code of equal power. The encoder is usually a simple circuit which has state memory and some feedback logic, normally XOR gates. The decoder can be implemented in software or firmware Unipolar vs bipolar When signaling over a communication link, there are two common binary signaling formats, unipolar and bipolar. A unipolar scheme is characterized by the voltage states being zero and +V volts, and thus has a dc component to the Fourier series expansion. A bipolar format has a zero dc mean with voltage states of +V and - V volts representing logic 1 and logic 0. It is shown that the bipolar format of signaling for data transmission is much more tolerant to noise than the unipolar equivalent for the same average symbol power. In the convolutional coding test, we use a unipolar to bipolar converter to test the Viterbi Decoder. <heading>3.4 State pinning, Puncturing and Cyclic Redundancy Check</heading>In computer science and information theory, the issue of error correction and detection has great practical importance. Error detection is the ability to detect errors that are made due to noise or other impairments in the course of the transmission from the transmitter to the receiver. Error correction has the additional feature that enables localization of the errors and correcting them. Given the goal of error correction, the idea of error detection may seem to be insufficient. However, error-correction schemes may be computationally intensive, or require excessive redundant data which may be inhibitive for a certain application. Error correction in some applications, such as a sender-receiver system (CRC), can be achieved with only a detection system in tandem with an automatic repeat request scheme to notify the sender that a portion of the data sent was received incorrectly and will need to be retransmitted, however where efficiency is important, it is possible to detect and correct errors with far less redundant data. The performance of the Viterbi Decoder can be improved if state pinning occurs such that the decoder knows a priori which state to traceback from irrespective of state metrics. This can be achieved by packetising the data, resetting the state of the encoder at the start of each packet and also appending a set of zero-valued pad-bits to the end of the packet to force the traceback from a given state. In the modeling process, we will use 8 pad bits giving us a packet of 120 data bits followed by 8 pad bits. The complexity of a Viterbi decoder increases rapidly with the code rate. Puncturing is a technique that allows the encoding and decoding of higher rate codes using standard rate 1/2 encoders and decoders. The Puncturing periodically removes bits from the encoded bit stream, thereby increasing the code rate. Puncturing also allows us to send a smaller sub-set of data, or rate match the input data rate to the available channel bandwidth and use the error-correction capability of the convolutional coder to add the redundancy while the convolutional decoder can estimate the most likely bit value. Many more complex error detection (and correction) methods make use of the properties of finite fields and polynomials over such fields. The cyclic redundancy check is a type of hash function used to produce a checksum. It considers a block of data as the coefficients to a polynomial and then divides by a fixed, predetermined polynomial. The coefficients of the result of the division are taken as the redundant data bits, the CRC. CRC contains blocks that append cyclic redundancy check (CRC) bits to data, and detect errors The function is that checking the received data can be achieved by multiplying the predetermined polynomial by the CRC. If this is the same as the payload data, then the data has been received without error. Alternatively, one can recompute the CRC from the payload bits and compare the CRC with the CRC that has been received. <table/>In CRC coding, the transmitter applies a rule to each message word to create extra, small fixed number of bits, called the checksum - against a block of data, such as a packet of network traffic and then append the checksum to the message word. The checksum is used to detect and correct errors after transmission or storage. Then CRC is computed and appended before transmission or storage, and verified afterwards by recipient to confirm that no changes occurred on transit. After receiving a transmitted word, the receiver applies the same rule to the received word. If the resulting checksum is nonzero, an error has occurred, and the transmitter should resend the message word. Cyclic redundancy check (CRC) coding is an error-control coding technique for detecting errors that occur when a message is transmitted. Unlike block or convolutional codes, CRC codes do not have a built-in error correction capability. Instead, when an error is detected in a received message word, the receiver requests the sender to retransmit the message word. CRCs are popular because they are simple to implement in binary hardware, are easy to analyze mathematically, and are particularly good at detecting common errors caused by noise in transmission channels. <heading>3.5 Multi-carrier modulation (OFDM) and Wireless LAN</heading>OFDM has become a very popular scheme implemented in DAB，DVB, 802.11a/g, ISDB, etc. It is a prime candidate for 4G.. In a radio environment, we see echos in the propagation path such that more than one copy of the transmitted signal is received creating constructive or destructive interference. However, each echo, and the main path will suffer various attenuation and phase shift factors. Echo delays may be long damaging many successive symbols causing Inter-Symbol Interference (ISI). For a time-domain signal, equalization is complex requiring large FIR Filters to counteract the ISI. In OFDM, we consider a frame of complex modulated signals to be information in the frequency with ascending sub-carrier frequency. Some pilot information is also added of known amplitude and phase. An IFFT is used to map this information to the time domain for transmission, termed an OFDM symbol. To separate successive symbols, a guard interval is added longer than the longest expected echo, which also allows the receiver to find the start of the OFDM symbol. Upon reception, an FFT is used to map the time domain symbol to the frequency domain. Since this is now frequency, each sub-carrier may have phase and amplitude distortion, but will not have changed frequency. Upon examining the relative phase and amplitude distortion of the received pilots, an estimate of the channel distortion is found as the received sub-carriers may be amplified and phase rotated back to create an estimate of what was transmitted. Orthogonal frequency-division multiplexing (OFDM), also sometimes called discrete multitone modulation (DMT), is a complex modulation technique for transmission based upon the idea of frequency-division multiplexing (FDM) where each frequency channel is modulated with a simpler modulation. In OFDM the frequencies and modulation of FDM are arranged to be orthogonal with each other which almost eliminates the interference between channels. In FDM, multiple signals are sent out at the same time, but on different frequencies. OFDM takes this concept further: In OFDM, a single transmitter transmits on many (typically dozens to thousands) different orthogonal frequencies (i.e. frequencies that are independent with respect to the relative phase relationship between the frequencies). Also, because the frequencies are so closely spaced, each one only has room for a Narrowband signal. An OFDM carrier signal is the sum of a number of orthogonal sub-carriers, with baseband data on each sub-carrier being independently modulated commonly using some type of QAM or PSK. This composite baseband signal is typically used to modulate a main RF carrier. This OFDM modulation technique coupled with the use of advanced modulation techniques on each component, results in a signal with high resistance to interference. The benefits of using OFDM are many, including high spectrum efficiency, resistance against multipath interference (particularly in wireless communications), and ease of filtering out noise (if a particular range of frequencies suffers from interference, the carriers within that range can be disabled or made to run slower). Also, the upstream and downstream speeds can be varied by allocating either more or fewer carriers for each purpose. Some forms of Rate-adaptive DSL use this feature in real time, so that bandwidth is allocated to whichever stream needs it most. Although the principles and some of the benefits have been known for 40 years, it is made popular today by the lower cost and availability of digital signal processing components. However, OFDM suffers from time-variations in the channel, or presence of a carrier frequency offset. This is due to the fact that the OFDM subcarriers are spaced closely in frequency. Imperfect frequency synchronization causes a loss in subcarrier orthogonality which severely degrades performance. Because the signal is the sum of a large number of subcarriers, it tends to have a high peak-to-average power ratio (PAPR). Also, it is necessary to minimise intermodulation between the subcarriers, which would effectively raise the noise floor both in-channel and out of channel. For this reason circuitry must be very linear. This is demanding, especially in relation to high power RF circuitry, which also needs to be efficient in order to minimise power consumption. <list><heading>OFDM feature abstract:</heading>No inter-carrier guard bands Maximum spectral efficiency (Nyquist rate) Easy implementation by FFTs Controlled overlapping of bands Very sensitive time-freq. synchronization </list><heading>Wireless Local Area Networks (LAN) </heading>OFDM is also now being used in some wireless LAN and MAN applications, including IEEE 802.11a/g (and the defunct European alternative HIPERLAN/2) and WiMAX. For amateur radio applications, experimental users have even hooked up commercial off-the-shelf ADSL equipment to radio transceivers which simply shift the bands used to the radio frequencies the user has licensed. IEEE 802.11a, operating in the 5 GHz band, specifies airside data rates ranging from 6 to 54 Mbit/s. Below contains a listing of the eight specified PHY data rates. Four different modulation schemes are used: BPSK, 4-QAM, 16-QAM, and 64-QAM. Each higher performing modulation scheme requires better channel condition for accurate transmission. These modulation schemes are coupled with the various forward error correction convolutional encoding schemes to give a multitude of Number of data bits per symbol (Ndbps) performance. <table/>Fadings are mathematical models for the distortion that a carrier-modulated telecommunication signal experiences over certain propagation media. Short term fading is due to multipath propagation, and is also known as multipath induced fading. Fading results from the superposition of transmitted signals which have experienced differences in attenuation, delay and phase shift while travelling from the source to the receiver. Flat fading is based on multipath time delay spread, where the bandwidth of the signal is less than the coherence bandwidth of the channel or the delay spread is less than the symbol period. The best way to combat fading is to ensure that multiple versions of the same signal are transmitted, received, and coherently combined. This is usually termed diversity, and sometimes acquired through multiple antennas. Mathematically, the simplest model for the fading phenomenon is multiplication of the signal waveform with a time-dependent coefficient which is often modeled as a random variable, making the received signal to noise ratio a random quantity. Fading channel models are often used to model electromagnetic transmission of information over wireless media such as cellular phones, and broadcast communications. However, even for underwater acoustic communications the notion of fading is useful in understanding the distortion caused by the medium. <heading>4． Implementation--------A process of modeling, testing together with comparisons and contrasts</heading>The steps in this section basically are as follows: <list>1. Building the Model2. Setting Parameters in the Model3. Running the Model4. Get the results and tabulate them for better analysis5. Compare and contrast the performances of different modulation schemes and various coding and error detection methods and their practical application in real communication systems (IEEE 802.11a). </list><heading>4.1 Working with the noise</heading>In the beginning, we are concerned with examining the behaviour of various noise sources to model the noise on digital communication using Matlab and Simulink. Set a model (model 1 in Appendix1 1), enter Mean=0.0 in the Random number block, start the simulation and notice that the running mean is close to 0. Change the variance with 2.0, 0, 0.5 and 1. We can model noise source using the Gaussian random number block. Record the values of running variance, mean and mean squared value of the noise. Here below is the result: <table/>From the results of this simulation we can notice that the running mean is always almost zero and easy to find that the variance in "display1" is very close to the variance entered in the random number block and the mean squared value of the noise is equal to the variance. The mean squared value of the noise is the noise power, which is equal to the variance here. And the Gaussian random number power is defined as its variance. Hence, we can model noise sources using the Gaussian random number. This 0-mean model here is good enough to simulate the random noise generated in Electronic systems. Set another model (model 3 in Appendix1 1. ) and note the variance and AWGN block output power for required SNR=0dB, 10dB, 20dB and 60dB and demonstrate the model is functioning correctly. <table/>From the table above, it is not difficult to find that the absolute value of required SNR and recovered SNR is almost equal to each other. We also find that the total signal power of the system (mean squared value) is almost equal to the sum of noise power (variance) and 1. Because the input signal= 1/sqrt(2)+j/sqrt(2)=0.707+j0.707, which has a normalised power of 1. So the total power of the system is composed of the input signal power, which is 1, and the noise power generated in AWGN channel, which can be read in the variance block. This is testified in from the result of the simulation. What's more,  FORMULA , therefore  FORMULA . So in the condition that the input power is a constant, the greater the SNR is, the smaller the noise power is. This result is also shown from the table. Take the example of  FORMULA :  FORMULA   FORMULA  They are almost equal to each other, which proved the formula and also indicated that the simulation was right. <heading>4.2 Digital modulation</heading>Now, we come to the digital modulation part. In this part, we will test different modulation schemes to see if they can work correctly and measure their Bit Error Rate (BER). First set a model (see in Appendix1 1. ). In this model, the Bernoulli Binary Generator block produces the information source, that is, random bits for this simulation. The BPSK Modulator Baseband modulates using the binary phase shift keying method. Its input must be a discrete-time binary-valued signal while output is a baseband representation of the modulated signal. The pair block BPSK demodulator which has a reverse function. Set the stop time to 2000 and start simulation, from the Discrete-Time Scatter Plot Scope, we got a scatter plot of BPSK on I-Q plane. (see in Appendix2 1.? ) Examine the source workspace variable and the mod workspace variable and check the mapping between them by comparing the input bits and output symbols. Select a set of data within a vector:  FORMULA , (i.e. display the first 15 elements). The result is shown in Appendix2 1, from which we can see that the source and the demod workplace variables are the same. Therefore it means that the output is conform to the input, that is, the modulator and demodulator work well. We also can learn that the mod modulate the bit "1" into -1.0000+0.00000i and the bit "0" into 1.0000. Then, change the modulator and demodulator blocks into QPSK, 16PSK and the 16QAM and 64QAM (see in Appendix1 1). In the same time, we should pay attention on the samples per frame, number of bits per integer, the value of M and phase offset, which should change in line with the different schemes. The scatter plots of these schemes can be found in  Appendix2 1. Now we have accomplished the testing of different modulation schemes and need to continue with the BER measuring with these methods. Add features to form a new model (see in Appendix1 1.) to measure the BER as a function of SNR. Remember: Always edit the working model in order to save time and avoid bugs. In this model, the Error Rate Calculation block compares the demodulated bits to the original source bits. The output of the Error Rate Calculation block is a three element vector containing the calculated BER, the number of error observed and the number of bits processed. The AWGN Channel transmits data, adding random numbers to simulate a noisy channel. Set the stop time to 1000000-1 to allow for the simulation of 1 million bits through the system. Start the simulation. Obtain the BER of the system for SNR values of 0,2,4,... 20dB. After finished BPSK, change the modulator and demodulator to QPSK, 16PSK, 16QAM and 64QAM likewise. Put all the results in a table and draw a graph of BER against SNR in Excel, including all the schemes. So we can compare and contrast them clearly. <table/>Table 4.3 Bit Error Rate Performance on different modulation schemes <figure/>From the Excel diagram 4.1, we can see the clear BER vs SNR performances in different modulation schemes. When the SNR is a fixed value, we hope smaller the BER the better. When the BER is a given value, we hope the required value of SNR to be smaller. It is obvious that the BPSK has the best performance according to the results in the diagram, the following are: QPSK, 16QAM, 16PSK and 64QAM. The reason why they have such differences in performance will be fully discussed in next session, which will include the complexity, the bit/symbol rates and error rates of these modulations and will trade off these aspects when facing different practical applications. <heading>4.3 Adding Convolutional Coding</heading>Up to data, we have implemented the modulation tests and research. The next big topic is about coding. As has introduced in the background part, we use the convolutional coding to protect the data transmitted by adding redundancy. So we add the convolutional encoder and Viterbi decoder in the model. Because Viterbi decoder does not accept bits but rather symbols allowing for puncturing, we introduce a unipolar to bipolar converter. First we will test the simple convolutional coder. Set a model (see in Appendix1). We find that the decoded data are the same as coded, but with a delay by 7 bits due to traceback delay. The result is in Appendix2, from which we know that the coding and decoding blocks work properly. Now that the convolutional coder is functioning, it is time to test the error correction capability of the convolutional coding scheme. Try  the simple convolutional coding scheme on the 16QAM with the simple convolutional coder. To do this, add convolutional coding to the 16QAM BER model. As such we are supplying a coded bit stream, rather than an uncoded bit stream to the modulator. The model is seen in Appendix1. The steps to obtain BER performance is the same as in modulation part. Start simulation, we will get the BER vs SNR results. Since the simple (3, [7 5]) scheme only has 4 states and is too simple, in practice we use the NASA (7, [171,133]) code. Again we get another group of data with the same16QAM. Likewise, change the scheme to BPSK, QPSK, 16PSK using the NASA code. Record their results carefully. Now the system becomes more and more complicated, looking rather cluttered. So we need to manage the model by adding Hierarchy. It makes sense to create 4 sub-systems off top hierarchical diagram being baseband coding, modulation, baseband decoding and demodulation. The models can be found in Appendix1 and the different sub-systems also can be found in Appendix1. As mentioned in the background research part, the performance of the Viterbi Decoder can be improved if state pinning occurs such that the decoder knows a priori which state to traceback from irrespective of state metrics. This can be achieved by packetising the data, resetting the state of the encoder at the start of each packet and also appending a set of zero-valued pad-bits to the end of the packet to force the traceback from a given state. To create this packet of data, edit the content of the Baseband Coding sub-system (see in Appendix1) to have two new components a constant block of value [00000000] and the Matrix Concatenation block configured with Number of Inputs set to 2 and Vertical Concatenation. We also need to remove the 8 pad bits from the decoded data. Note that the traceback depth in Viterbi decoder is 128 and the parameter of the Input port width is 128 while the Elements is [1: 120] in the selector after the decoder. Run the model now and note that the model runs much faster than before. Obtain the BER performance of the state-pinned 16QAM NASA convolutional coded model over SNR=0, 2, 4,... 20dB. Then, find the performances of the model using BPSK, QPSK and 16PSK. As we know, puncturing allows us to send a smaller sub-set of data, or rate match the input data rate to the available channel bandwidth and use the error-correction capability of the convolutional coder to add the redundancy while the convolutional decoder can estimate the most likely bit value. On the BPSK, QPSK, 16PSK and 16QAM packetised NASA coded models, add puncturing. This can be done by inserting the puncture block just after the convolutional coder and the insert zeros block just before the Viterbi Docoder. (See in Appendix1) The puncturing scheme we will use will remove 1 bit in 3 coded bits, therefore the overall code rate will change from 1/2 rate to 2/3. The puncture vector=[1110]' For each modulation scheme, obtain the performance of BER against SNR using the punctured with NASA convolutional code. The structure of this model can be specified block by block: Generating Random Data, Modulating, Convolutional Encoding, Puncturing, Transmitting Data, Demodulating, Inserting Zeros, Viterbi Decoding, Calculating the Error Rate and Evaluating Results. Next, we demonstrate each block's function in this simulation. (take the example of 16QAM) 1. Bernoulli Binary Generator creates random bits to use as message. It generates a frame of three random bits at each sample time. The Samples per frame parameter determines the number of rows of the output frame. 2. 16QAM Modulator Baseband modulate encoded message to prepare for transmission. 3. The Convolutional Encoder block encodes the data messages from the Bernoulli Binary Generator using the convolutional coding technique. 4. The Puncture block carries out the puncturing by periodically removing bits from the encoded bit stream, thereby increasing the code rate. 5. AWGN Channel transmits data, adding random numbers to simulate a noisy channel. 6. 16QAM Demodulator Baseband block demodulators a signal that was modulated using quadrature amplitude modulation with a constellation on a rectangular lattice. 7. Insert Zero insert zeros to substitute for bits removed by the Puncture block. The locations of the inserted zeros are determined by the Insert zero vector parameter in the mask., which is a binary column vector, usually the same as the puncture vector. Each 1 in the insert zero vector indicates that the block should place the next element of the input vector into the output vector (at the position of the 1). Each 0 indicates that the block should place a 0 into the output vector (at the position of the 0). This replaces the punctured bits with zeros. 8. In this simulation, the Viterbi Decoder block is set to accept unquantized inputs. The Viterbi Decoder block is configured to decode the same rate 1/2 code specified in the Convolutional Encoder block using the Viterbi algorithm. In this model, the decision type is set to Unquantized. For codes without puncturing, we will normally set the Traceback depth for this code to a value close to 40. However, for decoding punctured codes, a higher value is required to give the decoder enough data to resolve the ambiguities introduced by the inserted erasures. 9. Error Rate Calculation Compute the proportion of discrepancies between original and recovered messages. The Error Rate Calculation block compares the decoded bits to the original source bits. The output of the Error Rate Calculation block is a three-element vector containing the calculated bit error rate (BER), the number of errors observed, and the number of bits processed. BER simulations typically run until a minimum number of errors have occurred, or until the simulation processes a maximum number of bits. The Error Rate Calculation block uses its Stop simulation mode to set these limits and to control the duration of the simulation. Now we have finished all the coding part implementation and acquired lots of useful data. For example 16QAM, we have the 1.uncoded BER, 2.the simple coded, 3.the NASA coded, 4.the packetised NASA convolutional code and 5.the punctured NASA code. Hence, we can plot them on the graph for better comparison. For other schems, plot the 1.uncoded BER, 2.the packetised NASA convolutional code and 3.the punctured NASA code on the same plot. <table/><figure/><heading>BER Performance of BPSK using three coding schemes</heading><table/><figure/>Table 4.5 & Diagram 4.3 BER Performance of QPSK using three coding schemes <table/><figure/>From the above three groups of data and diagrams, we find that the best performance was made by the NASA coded with state-pin (pink). The NASA coded with puncturing is in the middle while the uncoded one has the worst performance. This is not hard to explain, but to discuss further, we will leave it to the next session. <table/><figure/>The pages before are the  comparison graphs among different coding schemes in a same modulation scheme. The  next one is the comparison among different modulation schemes in a same coding scheme------packetisd state-pinning NASA coding. The diagram is also shown in Appendix2 <table/><figure/><heading>4.4 Creating reliable packets</heading>We have noticed the BER depends on the SNR, for all schemes. If the received data can not accept any errors, by adding the CRC in the transported packet can guarantee to user that the data is reliable. At the receiver, the received data can be verified by re-calculating the CRC and checking that the whole packet passes the CRC check. If the packet passes then the packet is guaranteed error free and can be passed to higher protocol layers. If the packet fails the CRC (even though only one bit in error) then a packet fail can be indicated to the higher protocol layers - often resulting in a retransmission request. Therefore, in our system, the packer error rate(PER)--- Ratio of erased frames to total frames is required. The minimum acceptable PER for a given channel condition is stipulated in communication standards. To the packetised NASA coded 16QAM, add a standard 16CRC PER scheme: Change the samples per frame in the Bernoulli Binary Generator from 120 to 124, which will allow a 16 bit CRC to be added into the packet. Add the CRC-N Generator inside the baseband coding block before the Matrix Concatenation so that the packet is constricted from 104 bits of data, a 16 bit CRC and then 8 pad pits before convolutionally coded as before, see in Appendix1. The CRC-N Generator generates CRC bits according to CRC method and appends to input data frames. Place a CRC-N Syndrome Detector after the removal of the pad-bits in the baseband decoder sub-system with the same parameters as the CRC-N Generator, see in Appendix 1. The Detector has two outputs, the upper is the 104 output bits to go to the BER Error Rate Caculator and the lower is a value of '' if the packet was OK and '1' if the packet failed the CRC. So obviously the CRC-N Syndrome Detector detect errors in input data frames according to selected CRC method. In a similar fashion to the BER Error Rate Caculator, construct a PER Error Rate Calculator by comparing the pass/fail flag with '' abd display the results as BER and PER, see in Appendix 1. On the same plot, plot the PER for BPSK, QPSK, 16PSK and 16QAM over SNR 0,2,4,... 20dB over 1000000+6 bits. The result will be shown in the following page and also can be seen in Appendix 2. <heading>CRC BER</heading><table/><figure/><heading>CRC PER</heading><table/><figure/>Like the performance of uncoded scheme, the comparisons and contracts in both the BER and PER against SNR of different modulation schemes with the CRC method have a same conclusion, that is, the BPSK has the best performance, and then the QPSK and then 16QAM and the worst 16PSK. 4.5 Multi-carrier Modulation (OFDM) At last, we have accomplished all the modulation and convolutional coding parts. Now we come to the Multi-carrier modulation---OFDM scheme implementation. The OFDM model, offered by IEEE 802.11a (Wireless LAN), can simulate the data transmission in the radio environment. See in Appendix1 In the modulator bank (Appendix1), we can see 8 different modulation schemes selected dependant on the mode value and the NASA coding is used. The OFDM data, the pilots and the training sequences are collected together and the padding of 11 zeros is at the end of each frame and the cyclic prefix is added as well. The multi-path channel model allows you to s elect no multi-path(no fading), no echo but fading main channel(flat fading) and an echo channel(dispersive fading) all with AWGN. At the receiver, notice the cyclic prefix is removed and the FFT is performed to convert back from time to frequency. The frequency domain equalizer divides the received pilots by the sent pilots to extract an estimate of the channel frequency response and then equalizes the data by multiplying the received data symbols by an estimate of the inverted channel response. The demodulator bank performs the channel decoding. By re-modulating the decoded data symbols and comparing with the received symbols, an error vector can be created. An estimate of the channel SNR can be found and used to adaptively control the modulation used. Therefore, for poor channels BPSK can be used with a low data rate while noiseless channels allow 64QAM and a high data rate to be used. Run the model, first using the no fading propagation channel. By trial-and-error change the channel SNR and find out that the different modulation schemes are used for different SNR range with different achieved bit rates.  FORMULA   FORMULA   FORMULA   FORMULA   FORMULA  Using the no fading, flat fading and dispersive fading channel, for SNR=0, 2, ... 30 plot the PER against SNR and annotate the graph indicating which portion of the graph uses which modulation scheme and data rate for the three propagation channels. <table/><figure/><heading>Analysis and Discussion</heading>Up to now, we have gained all the materials and proofs for the further analysis and discussions. In PSK modulation schemes (BPSK, QPSK and 16PSK), the instantaneous power out of the modulator is always 1, because points on their constellation is equispaced around a circle therefore the amplitude is a constant 1. However, it is not a constant for QAM modulation. Take the example of 16QAM, it has three instantaneous power: 0.33333, , and1. Refer to the figures below. <figure/><figure/>The graph above compares the bit-error rates of BPSK, QPSK, 16QAM, 16PSK and 64QAM. As we have mentioned in implementation part that the level of performance from good to bad is: BPSK, QPSK, 16QAM, 16PSK and 64QAM. It is seen that higher-order modulations exhibit higher error-rates; in exchange however they deliver a higher raw data-rate. BPSK is the simplest form of PSK. It is This modulation  has the best immunity to noise of all the PSKs since it takes serious distortion to make the demodulator reach an incorrect decision. It is, however, only able to modulate at 1bit/symbol and so is unsuitable for high data-rate applications. With four phases, QPSK can encode two bits per symbol - twice the rate of BPSK. Analysis shows that this may be used either to double the data rate compared to a BPSK system while maintaining the bandwidth of the signal or to maintain the data-rate of BPSK but halve the bandwidth needed. Any number of phases may be used to construct a PSK constellation but 16-PSK is usually the highest order PSK constellation deployed. With more than 16 phases, the error-rate becomes too high and there are better, though more complex, modulations available such as quadrature amplitude modulation (QAM) after trading off the noise immunity, bandwidth and data rate. The reason of its better performance is that it achieves a greater distance between adjacent points in the I-Q plane by distributing the points more evenly. From the diagram we can see that the 16QAM modulation schemes perform 2 to 3 dB better than the 16PSK when the BER is the same, but it also pays the price that dealing with both the phase and amplitude. 64QAM has the biggest BER compared with other schemes in the same SNR. This is not hard to explain since it gets a notably high bit rate. It is suitable for the high data-rate less accuracy demanding applications. Another thing to point out is that the bit error rate is different from symbol error rate, though in BPSK they have same value. QPSK is sending two bits per symbol while 16PSK and 16QAM are 4 and the 64QAM is 6. The quantity of bits is always a power of 2 and the power is the equal number of bits per symbol. So except for BPSK, the symbol error rate (SER) of other modulation schemes is bigger than the bit error rate (BER) for they all have to send more than one bit per symbol. We can also see from the diagram that if the SNR is high, the probability of symbol error may be approximated and the BER will drop to acceptable level. And when SNR is low, no matter what kind of modulation schemes used, the BER is inevitably large. An OFDM carrier signal is the sum of a number of orthogonal sub-carriers, with baseband data on each sub-carrier being independently modulated commonly using some type of QAM or PSK. The upstream and downstream speeds can be varied by allocating either more or fewer carriers for each purpose. . <figure/>Comparing the three diagrams above, we find a similarity, that it the NASA coded with state-pinning (pink) is best in performance and then the one with puncturing (yellow) and the last is the uncoded one (blue). From the comparison it is easy to find that the performance of NASA coded scheme is improved by adding state-pinning. By knowing the starting and terminal states of the packetised data, the decoder can quickly locate where to start. The traceback starts at a given state from irrespective of state metrics. In the same time we noticed that the model also runs faster than previously because there is one traceback per packet not per bit now. The performance of NASA coded with puncturing is worse than the state-pinning one. By periodically removing bits from the encoded bit stream and inserting zeros to substitute for bits removed estimated by convolutional decoder, the chance of error increases as well as the code rate. But with puncturing, we can match the input data rate to the available channel bandwidth. The uncoded one absolutely has the worst performance since the data being transmitted don't have any error detection and protection. So they will be more susceptible to the noise. <figure/>Likewise, the 16QAM has a similar response as BPSK, QPSK and 16PSK to these coding schemes. <list>Best: Yellow: NASA coded with state-pinning 16QAM (with an improvement on Viterbi decoder knowing the priority to traceback)Second: Pink: NASA coded 16QAM (powerful convolutional coding)Third: Purple: simple coded 16QAM (simple but good enough coding scheme)Fourth: Green: NASA coded with puncturing 16QAM (rate match the input to the channel bandwidth and estimate the most likely value by convolutional decoder)Worst: Blue: uncoded 16QAM (worst) (no error detection and protection schemes)</list><figure/><figure/>Compare the BER and PER performance, we will find that they have similar results. BPSK has the best performance since that it is simple, powerful and has the strongest immunity to noises. The shortage is the slow data rate. The 16PSK has the high data rate but much more susceptible to noises. 16QAM has a better noise immunity compared to 16PSK considering they have a same symbol rate. Because it achieves a greater distance between adjacent points in the I-Q plane by distributing them more evenly. However, it has to handle both the phase and amplitude shifting and the transmission is more difficult than 16PSK due to the verifying power. So when it comes to the complexity of transmission of the system, the 16PSK is easier to implement. In the applications that doesn't require a demanding precision but preferring to the facility of transmission, we can choose 16PSK over 16QAM. <table/><table/>Contrast the CRC coding with the state-pinning NASA coding, we can still find a obvious improvement on the BER performance. Though the state-pinning NASA coding has the best performance among those coding schemes that introduced above, the CRC coding is superior to it. Because the CRC placed in the transported packet can detect errors and guarantee the reliable error free packet of data. From the diagram 4.8, we can find that for a notionally acceptance PER of 10%, the required SNR for the modulation schemes are about:  FORMULA   FORMULA   FORMULA   FORMULA  <figure/>From the diagram above, we can see a three different average PER vs SNR performances. No fading is that there is no noise and no echo in the system, flat fading is noise without echo while dispersive fading is noise with echo. So of course we will estimate that the performance will be the No fading best, flat fading second and dispersive fading last. In fact, the estimation is indeed true that the simulation and result have already shown. When the SNR is very small, which means a relatively weak signal, the PER will be very big. Even though the noise is almost zero, like the no fading system, the average PER still reaches 77dB. As a result, we should try to avoid the situation of weak signals and weak noises in real applications. <heading>6. Conclusion</heading>M-ray modulation schemes are more bandwidth efficient but more susceptible to noise compared with single-ary schemes. QPSK modulation is robust, but requires some form of linear amplification. 16QAM has the largest distance between points, but requires very linear amplification.16PSK has less stringent linearity requirements, but has less spacing between constellation points, and is therefore more affected by noise. Except for PSK and QAM, there are many other modulation schemes that can be used in the model, such as ASK, FSK and so on. All of them have their merits and disadvantages in application. However, PSK is often used as it provides a highly bandwidth efficient modulation scheme, which is suited to the limitation bandwidth of mobile radio systems. BPSK definitely will be chose as the modulation scheme for the system expecting reliable slow data transmission and 16QAM or 64QAM will be used in the faster but less reliable data communications. The system having simpler coding schemes sometimes can't guarantee the reliability so it needs a CRC detection and retransmission for failed packets. This method has some shortages since the receiver will drop the whole packet even though only one bit may be in error and CRC takes up some of the available bits in the packet. While the system has complicated coding schemes will reduce the error possibility but the coding will take longer time and requiring an higher capacity for the system however not 100% guarantee that the data transmitted are all error free. There are many problems in the wireless environment: noise, interference and the multi-path channel. In the presence of noise and interference, it is necessary to increase signal power to reduce the possibility of errors. In the multi-path channel, the received signal is made up of a sum of attenuated, phase shifted and time delayed versions of the transmitted signal by diffraction, transmission and reflections. The multi-path channels gives rise to irreducible errors from the Doppler effect of fading or inter-symbol interference. These errors are irreducible as they can not be removed by increasing signal power. Except for convolutional coding, there is another major coding called linear block code in channel coding. Such as Cyclic code (Hamming Code), Repetition code, Parity code, Reed Solomon code, BCH code, Reed Muller code, and Perfect code .etc. Other popular codes like ARQ code (automatic repeat request) and CDMA code (code division multiple address) are all widely used in common protocols: X25, TCP/IP and so on. Convolutional codes are more used in voiceband modems (V.32, V.17, V.34) and in GSM mobile phones, as well as satellite and military communication devices. Fundamentally, convolutional codes do not offer more protection against noise than an equivalent block code. In many cases, they generally offer greater simplicity of implementation over a block code of equal power. All the codes mentioned above can be added to the model here for simulation and compare their different performances with Viterbi convolutional code and conclude their various applications. In the end I will list some examples of using different coding schemes. The Internet with a typical TCP/IP stack uses a CRC-32 checksum error detection and the receiver discards frames if their checksums don't match. In the deep space telecommunication field, NASA has used many different error correcting codes. For missions between 1969 and 1977 the Mariner spacecraft used a Reed-Muller code. Color image transmission required 3 times the amount of data, so the Golay (24,12,8) code was used. Voyager 2 went on to Uranus and Neptune and the code was switched to a concatenated Reed-Solomon code-convolutional code for its substantially more powerful error correcting capabilities. In the satellite broadcasting (DVB) system, where is no retransmission possible, QPSK coupled with traditional Reed Solomon and Viterbi codes have been used for nearly 20 years for the delivery of digital satellite TV. Higher order modulation schemes such as 8PSK, 16QAM and 32QAM have enabled the satellite industry to increase transponder efficiency by several orders of magnitude. 